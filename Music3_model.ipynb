{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "Classify sounds using database\n",
    "Author: Scott H. Hawley\n",
    "This is kind of a mixture of Keun Woo Choi's code https://github.com/keunwoochoi/music-auto_tagging-keras\n",
    "   and the MNIST classifier at https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py\n",
    "Trained using Fraunhofer IDMT's database of monophonic guitar effects, \n",
    "   clips were 2 seconds long, sampled at 44100 Hz\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, TimeDistributed, LSTM, Dropout, Activation\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend\n",
    "from keras.utils import np_utils\n",
    "import os\n",
    "from os.path import isfile\n",
    "\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load music2_preproc.ipynb\n",
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"heading_collapsed\": true\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Setup\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 5,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"from __future__ import print_function\\n\",\n",
    "    \"\\n\",\n",
    "    \"''' \\n\",\n",
    "    \"Preprocess audio\\n\",\n",
    "    \"'''\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import librosa\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 6,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import librosa.display\\n\",\n",
    "    \"import os\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 7,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def get_class_names(path=\\\"Samples/\\\"):  # class names are subdirectory names in Samples/ directory\\n\",\n",
    "    \"    class_names = os.listdir(path)\\n\",\n",
    "    \"    print(class_names)\\n\",\n",
    "    \"    return class_names\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 8,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def preprocess_dataset(inpath=\\\"Samples/\\\", outpath=\\\"Preproc/\\\"):\\n\",\n",
    "    \"\\n\",\n",
    "    \"    if not os.path.exists(outpath):\\n\",\n",
    "    \"        os.mkdir( outpath, 0755 );   # make a new directory for preproc'd files\\n\",\n",
    "    \"\\n\",\n",
    "    \"    class_names = get_class_names(path=inpath)   # get the names of the subdirectories\\n\",\n",
    "    \"    nb_classes = len(class_names)\\n\",\n",
    "    \"    print(\\\"class_names = \\\",class_names)\\n\",\n",
    "    \"    for idx, classname in enumerate(class_names):   # go through the subdirs\\n\",\n",
    "    \"\\n\",\n",
    "    \"        if not os.path.exists(outpath+classname):\\n\",\n",
    "    \"            os.mkdir( outpath+classname, 0755 );   # make a new subdirectory for preproc class\\n\",\n",
    "    \"\\n\",\n",
    "    \"        class_files = os.listdir(inpath+classname)\\n\",\n",
    "    \"        n_files = len(class_files)\\n\",\n",
    "    \"        n_load = n_files\\n\",\n",
    "    \"        print(' class name = {:14s} - {:3d}'.format(classname,idx),\\n\",\n",
    "    \"            \\\", \\\",n_files,\\\" files in this class\\\",sep=\\\"\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"        printevery = 20\\n\",\n",
    "    \"        for idx2, infilename in enumerate(class_files):\\n\",\n",
    "    \"            audio_path = inpath + classname + '/' + infilename\\n\",\n",
    "    \"            if (0 == idx2 % printevery):\\n\",\n",
    "    \"                print('\\\\r Loading class: {:14s} ({:2d} of {:2d} classes)'.format(classname,idx+1,nb_classes),\\n\",\n",
    "    \"                       \\\", file \\\",idx2+1,\\\" of \\\",n_load,\\\": \\\",audio_path,sep=\\\"\\\")\\n\",\n",
    "    \"            #start = timer()\\n\",\n",
    "    \"            aud, sr = librosa.load(audio_path, sr=None)\\n\",\n",
    "    \"            melgram = librosa.logamplitude(librosa.feature.melspectrogram(aud, sr=sr, n_mels=96),ref_power=1.0)[np.newaxis,np.newaxis,:,:]\\n\",\n",
    "    \"            outfile = outpath + classname + '/' + infilename+'.npy'\\n\",\n",
    "    \"            np.save(outfile,melgram)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 2\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python2\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 2\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython2\",\n",
    "   \"version\": \"2.7.14\"\n",
    "  },\n",
    "  \"nav_menu\": {},\n",
    "  \"toc\": {\n",
    "   \"navigate_menu\": true,\n",
    "   \"number_sections\": true,\n",
    "   \"sideBar\": true,\n",
    "   \"threshold\": 6,\n",
    "   \"toc_cell\": false,\n",
    "   \"toc_section_display\": \"block\",\n",
    "   \"toc_window_display\": false\n",
    "  },\n",
    "  \"widgets\": {\n",
    "   \"state\": {},\n",
    "   \"version\": \"1.1.2\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mono=True\n",
    "\n",
    "\n",
    "def get_class_names(path=\"Preproc/\"):  # class names are subdirectory names in Preproc/ directory\n",
    "    class_names = os.listdir(path)\n",
    "    return class_names\n",
    "\n",
    "def get_total_files(path=\"Preproc/\",train_percentage=0.8): \n",
    "    sum_total = 0\n",
    "    sum_train = 0\n",
    "    sum_test = 0\n",
    "    subdirs = os.listdir(path)\n",
    "    for subdir in subdirs:\n",
    "        files = os.listdir(path+subdir)\n",
    "        n_files = len(files)\n",
    "        sum_total += n_files\n",
    "        n_train = int(train_percentage*n_files)\n",
    "        n_test = n_files - n_train\n",
    "        sum_train += n_train\n",
    "        sum_test += n_test\n",
    "    return sum_total, sum_train, sum_test\n",
    "\n",
    "def get_sample_dimensions(path='Preproc/'):\n",
    "    classname = os.listdir(path)[0]\n",
    "    files = os.listdir(path+classname)\n",
    "    infilename = files[0]\n",
    "    audio_path = path + classname + '/' + infilename\n",
    "    melgram = np.load(audio_path)\n",
    "    print(\"   get_sample_dimensions: melgram.shape = \",melgram.shape)\n",
    "    return melgram.shape\n",
    " \n",
    "\n",
    "def encode_class(class_name, class_names):  # makes a \"one-hot\" vector for each class name called\n",
    "    try:\n",
    "        idx = class_names.index(class_name)\n",
    "        vec = np.zeros(len(class_names))\n",
    "        vec[idx] = 1\n",
    "        return vec\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def shuffle_XY_paths(X,Y,paths):   # generates a randomized order, keeping X&Y(&paths) together\n",
    "    assert (X.shape[0] == Y.shape[0] )\n",
    "    idx = np.array(range(Y.shape[0]))\n",
    "    np.random.shuffle(idx)\n",
    "    newX = np.copy(X)\n",
    "    newY = np.copy(Y)\n",
    "    newpaths = paths\n",
    "    for i in range(len(idx)):\n",
    "        newX[i] = X[idx[i],:,:]\n",
    "        newY[i] = Y[idx[i],:]\n",
    "        newpaths[i] = paths[idx[i]]\n",
    "    return newX, newY, newpaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "So we make the training & testing datasets here, and we do it separately.\n",
    "Why not just make one big dataset, shuffle, and then split into train & test?\n",
    "because we want to make sure statistics in training & testing are as similar as possible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_datasets(train_percentage=0.8, preproc=False):\n",
    "    if (preproc):\n",
    "        path = \"Preproc/\"\n",
    "    else:\n",
    "        path = \"Samples/\"\n",
    "\n",
    "    class_names = get_class_names(path=path)\n",
    "    print(\"class_names = \",class_names)\n",
    "\n",
    "    total_files, total_train, total_test = get_total_files(path=path, train_percentage=train_percentage)\n",
    "    print(\"total files = \",total_files)\n",
    "\n",
    "    nb_classes = len(class_names)\n",
    "\n",
    "    # pre-allocate memory for speed (old method used np.concatenate, slow)\n",
    "    mel_dims = get_sample_dimensions(path=path)  # Find out the 'shape' of each data file\n",
    "    X_train = np.zeros((total_train, mel_dims[1], mel_dims[2], mel_dims[3]))   \n",
    "    Y_train = np.zeros((total_train, nb_classes))  \n",
    "    X_test = np.zeros((total_test, mel_dims[1], mel_dims[2], mel_dims[3]))  \n",
    "    Y_test = np.zeros((total_test, nb_classes))  \n",
    "    paths_train = []\n",
    "    paths_test = []\n",
    "\n",
    "    train_count = 0\n",
    "    test_count = 0\n",
    "    for idx, classname in enumerate(class_names):\n",
    "        this_Y = np.array(encode_class(classname,class_names) )\n",
    "        this_Y = this_Y[np.newaxis,:]\n",
    "        class_files = os.listdir(path+classname)\n",
    "        n_files = len(class_files)\n",
    "        n_load =  n_files\n",
    "        n_train = int(train_percentage * n_load)\n",
    "        printevery = 100\n",
    "        print(\"\")\n",
    "        for idx2, infilename in enumerate(class_files[0:n_load]):          \n",
    "            audio_path = path + classname + '/' + infilename\n",
    "            if (0 == idx2 % printevery):\n",
    "                print('\\r Loading class: {:14s} ({:2d} of {:2d} classes)'.format(classname,idx+1,nb_classes),\n",
    "                       \", file \",idx2+1,\" of \",n_load,\": \",audio_path,sep=\"\")\n",
    "            #start = timer()\n",
    "            if (preproc):\n",
    "              melgram = np.load(audio_path)\n",
    "              sr = 44100\n",
    "            else:\n",
    "              aud, sr = librosa.load(audio_path, mono=mono,sr=None)\n",
    "              melgram = librosa.logamplitude(librosa.feature.melspectrogram(aud, sr=sr, n_mels=96),ref_power=1.0)[np.newaxis,np.newaxis,:,:]\n",
    "\n",
    "            melgram = melgram[:,:,:,0:mel_dims[3]]   # just in case files are differnt sizes: clip to first file size\n",
    "       \n",
    "            #end = timer()\n",
    "            #print(\"time = \",end - start) \n",
    "            if (idx2 < n_train):\n",
    "                # concatenate is SLOW for big datasets; use pre-allocated instead\n",
    "                #X_train = np.concatenate((X_train, melgram), axis=0)  \n",
    "                #Y_train = np.concatenate((Y_train, this_Y), axis=0)\n",
    "                X_train[train_count,:,:] = melgram\n",
    "                Y_train[train_count,:] = this_Y\n",
    "                paths_train.append(audio_path)     # list-appending is still fast. (??)\n",
    "                train_count += 1\n",
    "            else:\n",
    "                X_test[test_count,:,:] = melgram\n",
    "                Y_test[test_count,:] = this_Y\n",
    "                #X_test = np.concatenate((X_test, melgram), axis=0)\n",
    "                #Y_test = np.concatenate((Y_test, this_Y), axis=0)\n",
    "                paths_test.append(audio_path)\n",
    "                test_count += 1\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: 'Preproc/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-569b3354e975>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_class_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-1d16594cd223>\u001b[0m in \u001b[0;36mget_class_names\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_class_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Preproc/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# class names are subdirectory names in Preproc/ directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: 'Preproc/'"
     ]
    }
   ],
   "source": [
    "get_class_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling order of data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-214d3ba39a4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shuffling order of data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle_XY_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle_XY_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "    print(\"Shuffling order of data...\")\n",
    "    X_train, Y_train, paths_train = shuffle_XY_paths(X_train, Y_train, paths_train)\n",
    "    X_test, Y_test, paths_test = shuffle_XY_paths(X_test, Y_test, paths_test)\n",
    "\n",
    "    return X_train, Y_train, paths_train, X_test, Y_test, paths_test, class_names, sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X,Y,nb_classes):\n",
    "    nb_filters = 32  # number of convolutional filters to use\n",
    "    pool_size = (2, 2)  # size of pooling area for max pooling\n",
    "    kernel_size = (3, 3)  # convolution kernel size\n",
    "    nb_layers = 4\n",
    "    input_shape = (1, X.shape[2], X.shape[3])\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n",
    "                        border_mode='valid', input_shape=input_shape))\n",
    "    model.add(BatchNormalization(axis=1, mode=2))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    for layer in range(nb_layers-1):\n",
    "        model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1]))\n",
    "        model.add(BatchNormalization(axis=1, mode=2))\n",
    "        model.add(ELU(alpha=1.0))  \n",
    "        model.add(MaxPooling2D(pool_size=pool_size))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # get the data\n",
    "    X_train, Y_train, paths_train, X_test, Y_test, paths_test, class_names, sr = build_datasets(preproc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # make the model\n",
    "    model = build_model(X_train,Y_train, nb_classes=len(class_names))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Initialize weights using checkpoint if it exists. (Checkpointing requires h5py)\n",
    "    load_checkpoint = True\n",
    "    checkpoint_filepath = 'weights.hdf5'\n",
    "    if (load_checkpoint):\n",
    "        print(\"Looking for previous weights...\")\n",
    "        if ( isfile(checkpoint_filepath) ):\n",
    "            print ('Checkpoint file detected. Loading weights.')\n",
    "            model.load_weights(checkpoint_filepath)\n",
    "        else:\n",
    "            print ('No checkpoint file detected.  Starting from scratch.')\n",
    "    else:\n",
    "        print('Starting from scratch (no checkpoint)')\n",
    "    checkpointer = ModelCheckpoint(filepath=checkpoint_filepath, verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # train and score the model\n",
    "    batch_size = 128\n",
    "    nb_epoch = 100\n",
    "    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          verbose=1, validation_data=(X_test, Y_test), callbacks=[checkpointer])\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
